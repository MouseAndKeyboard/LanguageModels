{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0864dc69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:09:59.246984Z",
     "start_time": "2022-05-29T14:09:57.749323Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "import pandas as pd\n",
    "import os\n",
    "from abc import ABC\n",
    "from gensim.corpora import Dictionary\n",
    "from torch.utils.data import Dataset\n",
    "import gensim.downloader as api\n",
    "from time import time\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm.notebook import tqdm\n",
    "from argparse import Namespace\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from Dataset import JobDescriptionDataset\n",
    "from Vectorizer import Word2vecVectoriser, OneHotVectoriser, PretrainedVectoriser\n",
    "\n",
    "%aimport Dataset\n",
    "%aimport Vectorizer\n",
    "\n",
    "#from 'Dataset' import JobDescriptionDataset\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import push_notebook, output_notebook\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import push_notebook, output_notebook\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f036491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:09:59.255879Z",
     "start_time": "2022-05-29T14:09:59.253860Z"
    }
   },
   "outputs": [],
   "source": [
    "# df, train, test,val = load_data(\"task2\", send_computer_to_hell=False)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36e269",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0009b3",
   "metadata": {},
   "source": [
    "### Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dbd9f9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:09:59.283628Z",
     "start_time": "2022-05-29T14:09:59.261380Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will\n",
    "    ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1bb778",
   "metadata": {},
   "source": [
    "### Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e812c754",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:09:59.500114Z",
     "start_time": "2022-05-29T14:09:59.488494Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "        \n",
    "def load_glove_from_file(glove_filepath):\n",
    "    \"\"\"\n",
    "    Load the GloVe embeddings \n",
    "    \n",
    "    Args:\n",
    "        glove_filepath (str): path to the glove embeddings file \n",
    "    Returns:\n",
    "        word_to_index (dict), embeddings (numpy.ndarary)\n",
    "    \"\"\"\n",
    "\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    with open(glove_filepath, encoding=\"utf8\") as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \") # each line: word num1 num2 ...\n",
    "            word_to_index[line[0]] = index # word = line[0] \n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "    return word_to_index, np.stack(embeddings)\n",
    "\n",
    "def make_embedding_matrix(glove_filepath, words):\n",
    "    \"\"\"\n",
    "    Create embedding matrix for a specific set of words.\n",
    "    \n",
    "    Args:\n",
    "        glove_filepath (str): file path to the glove embeddigns\n",
    "        words (list): list of words in the dataset\n",
    "    \"\"\"\n",
    "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n",
    "    embedding_size = glove_embeddings.shape[1]\n",
    "    \n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c507265c",
   "metadata": {},
   "source": [
    "## Vectorisers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8f7f4c",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b95cf03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:10:00.197656Z",
     "start_time": "2022-05-29T14:10:00.193746Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_word2vec():\n",
    "    df, train, test,val = load_data(\"task1\", send_computer_to_hell=False)\n",
    "\n",
    "    v = Word2vecVectoriser.from_dataframe(df)\n",
    "    print(v.num_features)\n",
    "    print(v.vectorize(df.tfidf10[0]))\n",
    "\n",
    "    v = Word2vecVectoriser.from_dataframe(df, data=\"job_description\", cutoff=0)\n",
    "    print(v.num_features)\n",
    "    print(v.vectorize(df.job_description[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec63d7ff",
   "metadata": {},
   "source": [
    "### One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78879b9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:10:00.511760Z",
     "start_time": "2022-05-29T14:10:00.505777Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_onehot():\n",
    "    df, train, test,val = load_data(\"task1\", send_computer_to_hell=False)\n",
    "\n",
    "    v = OneHotVectoriser.from_dataframe(df, is_sequence=False, data_field=\"tfidf10\", feature_field=\"is_fulltime\", sent_embed=False)\n",
    "    print(v.num_features)\n",
    "    print(v.vectorize(df.tfidf10[0]).shape)\n",
    "\n",
    "    v = OneHotVectoriser.from_dataframe(df, is_sequence=False, data_field=\"tfidf10\", feature_field=\"is_fulltime\", sent_embed=True)\n",
    "    print(v.num_features)\n",
    "    print(v.vectorize(df.tfidf10[0]).shape)\n",
    "\n",
    "    v = OneHotVectoriser.from_dataframe(df, is_sequence=True, data_field=\"tfidf10\", feature_field=\"is_fulltime\", sent_embed=False)\n",
    "    print(v.num_features)\n",
    "    print(v.vectorize(df.job_description[0]).shape)\n",
    "    \n",
    "    v = OneHotVectoriser.from_dataframe(df, is_sequence=True, data_field=\"job_description\", feature_field=\"is_fulltime\", sent_embed=False)\n",
    "    print(v.num_features)\n",
    "    print(v.vectorize(df.job_description[0]).shape)\n",
    "\n",
    "    v = OneHotVectoriser.from_dataframe(df, is_sequence=True, data_field=\"job_description\", feature_field=\"is_fulltime\", sent_embed=True)\n",
    "    print(v.num_features)\n",
    "    print(v.vectorize(df.job_description[0]).shape)\n",
    "\n",
    "# test_onehot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d5ef7",
   "metadata": {},
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ab171c",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9fbce4",
   "metadata": {},
   "source": [
    "### Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9bdad48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:10:01.117894Z",
     "start_time": "2022-05-29T14:10:01.112809Z"
    }
   },
   "outputs": [],
   "source": [
    "class JobDescriptionNotSimpleClassifier(nn.Module):\n",
    "    \"\"\" a simple perceptron-based classifier \"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int): the size of the input feature vector\n",
    "        \"\"\"\n",
    "        super(JobDescriptionNotSimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features, out_features=int(num_features/2))\n",
    "        self.fc2 = nn.Linear(in_features=int(num_features/2), out_features=1)\n",
    "    \n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor \n",
    "                    x_in.shape should be (batch, num_features)\n",
    "            apply_sigmoid (bool): a flag for the sigmoid activation\n",
    "                    should be false if used with the cross-entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch,).\n",
    "        \"\"\"\n",
    "        x_in = self.fc1(x_in)\n",
    "        x_in = F.relu(x_in)\n",
    "        y_out = self.fc2(x_in).squeeze()\n",
    "        \n",
    "        if apply_sigmoid:\n",
    "            y_out = torch.sigmoid(y_out)\n",
    "            \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb1a1aeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:10:01.235855Z",
     "start_time": "2022-05-29T14:10:01.231831Z"
    }
   },
   "outputs": [],
   "source": [
    "class JobDescriptionSimpleClassifier(nn.Module):\n",
    "    \"\"\" a simple perceptron-based classifier \"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int): the size of the input feature vector\n",
    "        \"\"\"\n",
    "        super(JobDescriptionSimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features, out_features=1)\n",
    "    \n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor \n",
    "                    x_in.shape should be (batch, num_features)\n",
    "            apply_sigmoid (bool): a flag for the sigmoid activation\n",
    "                    should be false if used with the cross-entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch,).\n",
    "        \"\"\"\n",
    "        y_out = self.fc1(x_in).squeeze()\n",
    "        \n",
    "        if apply_sigmoid:\n",
    "            y_out = torch.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f516ac",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b1c8a6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:10:01.544991Z",
     "start_time": "2022-05-29T14:10:01.530420Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_FWD_train(args):\n",
    "    def make_train_state(args):\n",
    "        return {'epoch_index': 0,\n",
    "                'train_loss': [],\n",
    "                'train_acc': [],\n",
    "                'val_loss': [],\n",
    "                'val_acc': [],\n",
    "                'test_loss': 1,\n",
    "                'test_acc': 1}\n",
    "\n",
    "    train_state = make_train_state(args)\n",
    "    if not torch.cuda.is_available():\n",
    "        args.cuda = False\n",
    "    args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "    # dataset and vectorizer\n",
    "    # dataset2 = JobDescriptionDataset.load_dataset_and_make_vectorizer(args.input_name, Word2vecVectoriser)\n",
    "    # vectorizer2 = dataset2.get_vectorizer()\n",
    "    dataset = JobDescriptionDataset.load_dataset_and_make_vectorizer(args.name, args.vectoriser, args.data_field, args.feature_field, args.is_sequence, args.sent_embed)\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    print(vectorizer)\n",
    "    # model\n",
    "    # classifier = JobDescriptionClassifier(num_features=len(vectorizer.input_vocab))\n",
    "    classifier = JobDescriptionNotSimpleClassifier(num_features=vectorizer.num_features)\n",
    "    classifier = classifier.to(args.device)\n",
    "    # loss and optimizer\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    def compute_accuracy(y_pred, y_target):\n",
    "        y_target = y_target.cpu()\n",
    "        y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "        n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "        return n_correct / len(y_pred_indices) * 100\n",
    "    \n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "        # Iterate over training dataset\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "        for batch_index, batch_dict in tqdm(enumerate(batch_generator), colour='white'):\n",
    "            # the training routine is 5 steps:\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_batch = loss.item()\n",
    "            running_loss += (loss_batch-running_loss) / (batch_index + 1)\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # compute the accuracy\n",
    "            acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "        # setup: batch generator, set loss and acc to 0, set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # step 1. compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "            # step 2. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_batch = loss.item()\n",
    "            running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "            # step 3. compute the accuracy\n",
    "            acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "        \n",
    "    dataset.set_split('test')\n",
    "    batch_generator = generate_batches(dataset,batch_size=args.batch_size,device=args.device)\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # compute the output\n",
    "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "        # compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "        # compute the accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "    train_state['test_loss'] = running_loss\n",
    "    train_state['test_acc'] = running_acc\n",
    "    \n",
    "    print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\n",
    "    print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab2f0045",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:10:01.692375Z",
     "start_time": "2022-05-29T14:10:01.688357Z"
    }
   },
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Data and path information\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file='task1-model.pth',\n",
    "    name='task1',\n",
    "    save_dir='./model_storage/task1/',\n",
    "    vectorizer_file='vectorizer.json',\n",
    "    # No model hyperparameters\n",
    "    # Training hyperparameters\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.01,\n",
    "    num_epochs=5,\n",
    "    seed=1337,\n",
    "    # Runtime options\n",
    "    cuda=True,\n",
    "    device='cuda',\n",
    "    vectoriser=PretrainedVectoriser,\n",
    "    data_field='tfidf10',\n",
    "    feature_field=\"is_fulltime\",\n",
    "    is_sequence=False,\n",
    "    sent_embed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20562fc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:11:02.255346Z",
     "start_time": "2022-05-29T14:10:01.839894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vectorizer.PretrainedVectoriser object at 0x7f593128d300>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e732f64e54ab4cb591a386a2a24fc20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2fbacbac6714f58b4a6e29ea3a87021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dca184179fe48868ff3698e52985b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c2702a7f774023a45f8ea828fd22ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b17c9a68a81403ab123392befd60836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.534\n",
      "Test Accuracy: 73.68\n"
     ]
    }
   ],
   "source": [
    "do_FWD_train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6dbd59c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:11:03.444950Z",
     "start_time": "2022-05-29T14:11:02.258808Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_train_state(args):\n",
    "    return {'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': 1,\n",
    "            'test_acc': 1}\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# dataset and vectorizer\n",
    "# dataset2 = JobDescriptionDataset.load_dataset_and_make_vectorizer(args.input_name, Word2vecVectoriser)\n",
    "# vectorizer2 = dataset2.get_vectorizer()\n",
    "dataset = JobDescriptionDataset.load_dataset_and_make_vectorizer(args.name, OneHotVectoriser, 'tfidf10', 'is_fulltime', False, True)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "# model\n",
    "# classifier = JobDescriptionClassifier(num_features=len(vectorizer.input_vocab))\n",
    "classifier = JobDescriptionNotSimpleClassifier(num_features=vectorizer.num_features)\n",
    "classifier = classifier.to(args.device)\n",
    "# loss and optimizer\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a3b9e",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "734fe3ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:11:10.079494Z",
     "start_time": "2022-05-29T14:11:03.448621Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991034cbd60c4304805c9b334e890d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b945fec02e6405fb40c82b523cf43ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b84ad2b45da48b9a8d3d51bcb0406c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238aaabb76e44a979b1480cc9a8dfe96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335c9a6d55fe42e9988688962a623c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "    # Iterate over training dataset\n",
    "    # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "    for batch_index, batch_dict in tqdm(enumerate(batch_generator), colour='white'):\n",
    "        # the training routine is 5 steps:\n",
    "        # step 1. zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # step 2. compute the output\n",
    "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch-running_loss) / (batch_index + 1)\n",
    "        # step 4. use loss to produce gradients\n",
    "        loss.backward()\n",
    "        # step 5. use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        # compute the accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "    # Iterate over val dataset\n",
    "    # setup: batch generator, set loss and acc to 0, set eval mode on\n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # step 1. compute the output\n",
    "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "        # step 2. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "        # step 3. compute the accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0284b571",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "046d1aa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:11:10.222559Z",
     "start_time": "2022-05-29T14:11:10.087628Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset,batch_size=args.batch_size,device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_batch = loss.item()\n",
    "    running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "    # compute the accuracy\n",
    "    acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f5b78f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:11:10.231783Z",
     "start_time": "2022-05-29T14:11:10.230030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.387\n",
      "Test Accuracy: 86.00\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f4a628",
   "metadata": {},
   "source": [
    "### CNN Conv1D Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "23ca77f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:43:18.875739Z",
     "start_time": "2022-05-29T14:43:18.871253Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNNEmbedClassifier(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings, num_channels, \n",
    "                 hidden_dim, num_classes, dropout_p, \n",
    "                 pretrained_embeddings=None, padding_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size (int): size of the embedding vectors\n",
    "            num_embeddings (int): number of embedding vectors\n",
    "            filter_width (int): width of the convolutional kernels\n",
    "            num_channels (int): number of convolutional kernels per layer\n",
    "            hidden_dim (int): the size of the hidden dimension\n",
    "            num_classes (int): the number of classes in classification\n",
    "            dropout_p (float): a dropout parameter \n",
    "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
    "                default is None. If provided, \n",
    "            padding_idx (int): an index representing a null position\n",
    "        \"\"\"\n",
    "        super(CNNEmbedClassifier, self).__init__()\n",
    "\n",
    "        if pretrained_embeddings is None:\n",
    "\n",
    "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
    "                                    num_embeddings=num_embeddings,\n",
    "                                    padding_idx=padding_idx)        \n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
    "                                    num_embeddings=num_embeddings,\n",
    "                                    padding_idx=padding_idx,\n",
    "                                    _weight=pretrained_embeddings)\n",
    "        \n",
    "            \n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_size, \n",
    "                   out_channels=num_channels, kernel_size=1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                   kernel_size=2, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                   kernel_size=2, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                   kernel_size=2),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self._dropout_p = dropout_p\n",
    "        self.fc1 = nn.Linear(num_channels, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (batch, dataset._max_seq_length)\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
    "        \"\"\"\n",
    "\n",
    "#         print(x_in.shape)\n",
    "        # embed and permute so features are channels\n",
    "        x_embedded = self.emb(x_in)\n",
    "#         print(x_embedded.shape)\n",
    "        x_embedded = x_embedded.permute(0, 2, 1)\n",
    "#         print(x_embedded.shape)\n",
    "        features = self.convnet(x_embedded)\n",
    "#         print(features.shape)\n",
    "        # average and remove the extra dimension\n",
    "        remaining_size = features.size(dim=2)\n",
    "#         print(remaining_size)\n",
    "        features = F.avg_pool1d(features, remaining_size).squeeze(dim=2)\n",
    "#         print(features.shape)\n",
    "        features = F.dropout(features, p=self._dropout_p)\n",
    "        \n",
    "        # mlp classifier\n",
    "        intermediate_vector = F.relu(F.dropout(self.fc1(features), p=self._dropout_p))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e7d944f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:42:10.959359Z",
     "start_time": "2022-05-29T14:42:10.954964Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings, num_channels, \n",
    "                 hidden_dim, num_classes, dropout_p, \n",
    "                 pretrained_embeddings=None, padding_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size (int): size of the embedding vectors\n",
    "            num_embeddings (int): number of embedding vectors\n",
    "            filter_width (int): width of the convolutional kernels\n",
    "            num_channels (int): number of convolutional kernels per layer\n",
    "            hidden_dim (int): the size of the hidden dimension\n",
    "            num_classes (int): the number of classes in classification\n",
    "            dropout_p (float): a dropout parameter \n",
    "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
    "                default is None. If provided, \n",
    "            padding_idx (int): an index representing a null position\n",
    "        \"\"\"\n",
    "        super(CNNClassifier, self).__init__()\n",
    "\n",
    "            \n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_size, \n",
    "                   out_channels=num_channels, kernel_size=1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                   kernel_size=2, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                   kernel_size=2, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                   kernel_size=2),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self._dropout_p = dropout_p\n",
    "        self.fc1 = nn.Linear(num_channels, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (batch, dataset._max_seq_length)\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
    "        \"\"\"\n",
    "        # embed and permute so features are channels\n",
    "#         x_embedded = self.emb(x_in).permute(0, 2, 1)\n",
    "#         print(x_in) # torch.Size([100, 10, 100])\n",
    "#         print(x_in.shape)\n",
    "#         torch.Size([128, 12])\n",
    "#         torch.Size([128, 12, 100])\n",
    "#         torch.Size([128, 100, 12])\n",
    "#         torch.Size([128, 100, 2])\n",
    "#         2\n",
    "#         torch.Size([128, 100])\n",
    "\n",
    "#         print(x_in.shape)\n",
    "#         # embed and permute so features are channels\n",
    "#         x_embedded = self.emb(x_in)\n",
    "#         print(x_embedded.shape)\n",
    "#         x_embedded = x_embedded.permute(0, 2, 1)\n",
    "#         print(x_embedded.shape)\n",
    "#         features = self.convnet(x_embedded)\n",
    "#         print(features.shape)\n",
    "#         # average and remove the extra dimension\n",
    "#         remaining_size = features.size(dim=2)\n",
    "#         print(remaining_size)\n",
    "#         features = F.avg_pool1d(features, remaining_size).squeeze(dim=2)\n",
    "#         print(features.shape)\n",
    "#         features = F.dropout(features, p=self._dropout_p)\n",
    "\n",
    "\n",
    "\n",
    "        features = self.convnet(x_in.permute(0,2,1))\n",
    "#         print(features.shape)\n",
    "#         print(features)\n",
    "#         features = features.permute(0, 2, 1)\n",
    "        # average and remove the extra dimension\n",
    "        remaining_size = features.size(dim=2)\n",
    "        features = F.avg_pool1d(features, remaining_size).squeeze(dim=2)\n",
    "        features = F.dropout(features, p=self._dropout_p)\n",
    "        \n",
    "        # mlp classifier\n",
    "        intermediate_vector = F.relu(F.dropout(self.fc1(features), p=self._dropout_p))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c0e9e8",
   "metadata": {},
   "source": [
    "#### Train Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d022cde3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:42:14.029284Z",
     "start_time": "2022-05-29T14:42:14.025351Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccaae0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T07:53:28.562921Z",
     "start_time": "2022-05-29T07:53:28.558454Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## Do Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d5a2fb2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:42:16.239373Z",
     "start_time": "2022-05-29T14:42:16.229467Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def do_train(args):\n",
    "    if args.expand_filepaths_to_save_dir:\n",
    "        args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                            args.vectorizer_file)\n",
    "\n",
    "        args.model_state_file = os.path.join(args.save_dir,\n",
    "                                             args.model_state_file)\n",
    "\n",
    "        print(\"Expanded filepaths: \")\n",
    "        print(\"\\t{}\".format(args.vectorizer_file))\n",
    "        print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "    # Check CUDA\n",
    "    if not torch.cuda.is_available():\n",
    "        args.cuda = False\n",
    "\n",
    "    args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "    # handle dirs\n",
    "    handle_dirs(args.save_dir)\n",
    "    # args.use_glove = True\n",
    "\n",
    "    if args.reload_from_files:\n",
    "        # training from a checkpoint\n",
    "        dataset = NewsDataset.load_dataset_and_load_vectorizer(args.news_csv,\n",
    "                                                               args.vectorizer_file)\n",
    "    else:\n",
    "        dataset = JobDescriptionDataset.load_dataset_and_make_vectorizer(args.name, args.vectoriser,  args.data_field, args.feature_field, \n",
    "            args.is_sequence, args.sent_embed)\n",
    "        # create dataset and vectorizer\n",
    "        # dataset = JobDescriptionDataset.load_dataset_and_make_vectorizer(args.name, vectoriser=CNNVectorizer)\n",
    "        # dataset.save_vectorizer(args.vectorizer_file)\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "    # Use GloVe or randomly initialized embeddings\n",
    "    if args.use_glove:\n",
    "        words = vectorizer.input_vocab._token_to_idx.keys()\n",
    "        embeddings = make_embedding_matrix(glove_filepath=args.glove_filepath, \n",
    "                                           words=words)\n",
    "        print(\"Using pre-trained embeddings\")\n",
    "    else:\n",
    "        print(\"Not using pre-trained embeddings\")\n",
    "        embeddings = None\n",
    "\n",
    "\n",
    "    classifier = args.classifier(embedding_size=args.embedding_size, \n",
    "                                num_embeddings=len(vectorizer.input_vocab),\n",
    "                                num_channels=args.num_channels,\n",
    "                                hidden_dim=args.hidden_dim, \n",
    "                                num_classes=len(vectorizer.class_vocab), \n",
    "                                dropout_p=args.dropout_p,\n",
    "                                pretrained_embeddings=embeddings,\n",
    "                                padding_idx=0)\n",
    "    \n",
    "    classifier = classifier.to(args.device)\n",
    "    dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                               mode='min', factor=0.5,\n",
    "                                               patience=1)\n",
    "\n",
    "    train_state = make_train_state(args)\n",
    "\n",
    "    epoch_bar = tqdm(desc='training routine', \n",
    "                              total=args.num_epochs,\n",
    "                              position=0)\n",
    "\n",
    "    dataset.set_split('train')\n",
    "    train_bar = tqdm(desc='split=train',\n",
    "                              total=dataset.get_num_batches(args.batch_size), \n",
    "                              position=1, \n",
    "                              leave=True)\n",
    "    dataset.set_split('val')\n",
    "    val_bar = tqdm(desc='split=val',\n",
    "                            total=dataset.get_num_batches(args.batch_size), \n",
    "                            position=1, \n",
    "                            leave=True)\n",
    "\n",
    "\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "    # for epoch_index in range(2):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "    #         print(batch_dict['x_data'])\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # compute the output\n",
    "            y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "        \n",
    "    # compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "    classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "    classifier = classifier.to(args.device)\n",
    "    dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "    loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "    dataset.set_split('test')\n",
    "    batch_generator = generate_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       device=args.device)\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # compute the output\n",
    "        y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "        # compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "    train_state['test_loss'] = running_loss\n",
    "    train_state['test_acc'] = running_acc\n",
    "\n",
    "    print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "    print(\"Test Accuracy: {}\".format(train_state['test_acc']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5555ee94",
   "metadata": {},
   "source": [
    "## One hot embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ec82d44a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:43:18.902333Z",
     "start_time": "2022-05-29T14:43:18.883938Z"
    }
   },
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    name=\"task1\",\n",
    "    news_csv=\"./data/ag_news/news_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"./model_storage/document_classification\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath='/home/dadams/Files/uni/nlp/data/glove.6B/glove.6B.100d.txt', \n",
    "    use_glove=False,\n",
    "    embedding_size=100, \n",
    "    hidden_dim=100, \n",
    "    num_channels=100, \n",
    "    # Training hyper parameter\n",
    "    seed=1337, \n",
    "    learning_rate=0.001, \n",
    "    dropout_p=0.1, \n",
    "    batch_size=128, \n",
    "    num_epochs=1, \n",
    "    early_stopping_criteria=5, \n",
    "    # Runtime option\n",
    "    cuda=True, \n",
    "    catch_keyboard_interrupt=True, \n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    # Dataset params\n",
    "    data_field='tfidf10', \n",
    "    feature_field='is_fulltime', \n",
    "    is_sequence=True, \n",
    "    sent_embed=True,\n",
    "    vectoriser=OneHotVectoriser,\n",
    "    classifier=CNNEmbedClassifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3cd097ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:43:22.121254Z",
     "start_time": "2022-05-29T14:43:18.910053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\t./model_storage/document_classification/vectorizer.json\n",
      "\t./model_storage/document_classification/model.pth\n",
      "Using CUDA: True\n",
      "Not using pre-trained embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ecbf5092894b378fda0662ab1d1eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39d9cfed31d4d3a85e7404a2742a4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3f1851046748338f7e77b09afc25e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6378262088848994;\n",
      "Test Accuracy: 63.76201923076923\n"
     ]
    }
   ],
   "source": [
    "do_train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1074923",
   "metadata": {},
   "source": [
    "## Other Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a0beecc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:42:54.289653Z",
     "start_time": "2022-05-29T14:42:21.592558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\t./model_storage/document_classification/vectorizer.json\n",
      "\t./model_storage/document_classification/model.pth\n",
      "Using CUDA: True\n",
      "Not using pre-trained embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd3848aeba5422eac23027fe6f78b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7222eafa6fd04e8eb01dae6317d28457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af858ea070c4d61988a5516a6d7a2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5708228647708893;\n",
      "Test Accuracy: 71.11764705882354\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    name=\"task1\",\n",
    "    news_csv=\"./data/ag_news/news_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"./model_storage/document_classification\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath='/home/dadams/Files/uni/nlp/data/glove.6B/glove.6B.100d.txt', \n",
    "    use_glove=False,\n",
    "    embedding_size=300, \n",
    "    hidden_dim=100, \n",
    "    num_channels=100, \n",
    "    # Training hyper parameter\n",
    "    seed=1337, \n",
    "    learning_rate=0.001, \n",
    "    dropout_p=0.1, \n",
    "    batch_size=100, \n",
    "    num_epochs=5, \n",
    "    early_stopping_criteria=5, \n",
    "    # Runtime option\n",
    "    cuda=True, \n",
    "    catch_keyboard_interrupt=True, \n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    # Dataset params\n",
    "    data_field='tfidf10', \n",
    "    feature_field='is_fulltime', \n",
    "    is_sequence=True, \n",
    "    sent_embed=False,\n",
    "    vectoriser=PretrainedVectoriser,\n",
    "    classifier=CNNClassifier\n",
    ")\n",
    "do_train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13593ad",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "29bf1fd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:43:18.846948Z",
     "start_time": "2022-05-29T14:42:54.298700Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\t./model_storage/document_classification/./model_storage/document_classification/vectorizer.json\n",
      "\t./model_storage/document_classification/./model_storage/document_classification/model.pth\n",
      "Using CUDA: True\n",
      "Not using pre-trained embeddings\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)\n",
    "# args.use_glove = True\n",
    "\n",
    "if args.reload_from_files:\n",
    "    # training from a checkpoint\n",
    "    dataset = NewsDataset.load_dataset_and_load_vectorizer(args.news_csv,\n",
    "                                                           args.vectorizer_file)\n",
    "else:\n",
    "    dataset = JobDescriptionDataset.load_dataset_and_make_vectorizer(args.name, args.vectoriser, args.data_field, args.feature_field, \n",
    "        args.is_sequence, args.sent_embed)\n",
    "    # create dataset and vectorizer\n",
    "    # dataset = JobDescriptionDataset.load_dataset_and_make_vectorizer(args.name, vectoriser=CNNVectorizer)\n",
    "    # dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# Use GloVe or randomly initialized embeddings\n",
    "if args.use_glove:\n",
    "    words = vectorizer.input_vocab._token_to_idx.keys()\n",
    "    embeddings = make_embedding_matrix(glove_filepath=args.glove_filepath, \n",
    "                                       words=words)\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "else:\n",
    "    print(\"Not using pre-trained embeddings\")\n",
    "    embeddings = None\n",
    "\n",
    "\n",
    "print(len(vectorizer.class_vocab))\n",
    "\n",
    "classifier = CNNClassifier(embedding_size=args.embedding_size, \n",
    "                            num_embeddings=len(vectorizer.input_vocab),\n",
    "                            num_channels=args.num_channels,\n",
    "                            hidden_dim=args.hidden_dim, \n",
    "                            num_classes=len(vectorizer.class_vocab), \n",
    "                            dropout_p=args.dropout_p,\n",
    "                            pretrained_embeddings=embeddings,\n",
    "                            padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae69a486",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:14:25.719361Z",
     "start_time": "2022-05-29T14:14:25.717026Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vectorize(dataset.df.tfidf10[0])[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947e57cb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9315be73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T07:57:32.381438Z",
     "start_time": "2022-05-29T07:57:32.065329Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a759bd52e249cf86428cee612c2da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343756dbd1f84437b6c76a6a73a2091a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a1657a1410442b92658b99938438c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.DoubleTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#         print(batch_dict['x_data'])\u001b[39;00m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;66;03m# step 2. compute the output\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m# step 3. compute the loss\u001b[39;00m\n\u001b[1;32m     54\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_func(y_pred, batch_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_target\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.conda/envs/nlp4/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36mCNNClassifier.forward\u001b[0;34m(self, x_in, apply_softmax)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m\"\"\"The forward pass of the classifier\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    the resulting tensor. tensor.shape should be (batch, num_classes)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# embed and permute so features are channels\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m x_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_in\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvnet(x_embedded)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# average and remove the extra dimension\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nlp4/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/nlp4/lib/python3.10/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nlp4/lib/python3.10/site-packages/torch/nn/functional.py:2183\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2177\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2178\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2179\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2180\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2181\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.DoubleTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "    \n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "\n",
    "for epoch_index in range(args.num_epochs):\n",
    "# for epoch_index in range(2):\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "    # Iterate over training dataset\n",
    "\n",
    "    # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       device=args.device)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # the training routine is these 5 steps:\n",
    "\n",
    "        # --------------------------------------\n",
    "        # step 1. zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "#         print(batch_dict['x_data'])\n",
    "        # step 2. compute the output\n",
    "        y_pred = classifier(batch_dict['x_data'])\n",
    "        \n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # step 4. use loss to produce gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # step 5. use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        # -----------------------------------------\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        # update bar\n",
    "        train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                              epoch=epoch_index)\n",
    "        train_bar.update()\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "    # Iterate over val dataset\n",
    "\n",
    "    # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       device=args.device)\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "        # compute the output\n",
    "        y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "        val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                        epoch=epoch_index)\n",
    "        val_bar.update()\n",
    "\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "\n",
    "    train_state = update_train_state(args=args, model=classifier,\n",
    "                                     train_state=train_state)\n",
    "\n",
    "    scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "    if train_state['stop_early']:\n",
    "        break\n",
    "\n",
    "    train_bar.n = 0\n",
    "    val_bar.n = 0\n",
    "    epoch_bar.update()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e16358",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae7817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T04:00:55.074566Z",
     "start_time": "2022-05-29T04:00:54.526520Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred =  classifier(batch_dict['x_data'])\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7441eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T04:00:55.861851Z",
     "start_time": "2022-05-29T04:00:55.858530Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50154c18",
   "metadata": {},
   "source": [
    "### CNN Conv1D (full job description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8eae8f3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:43:22.553601Z",
     "start_time": "2022-05-29T14:43:22.551000Z"
    }
   },
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    name=\"task1\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/document_classification\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath='/home/dadams/Files/uni/nlp/data/glove.6B/glove.6B.100d.txt', \n",
    "    use_glove=False,\n",
    "    embedding_size=100, \n",
    "    hidden_dim=100, \n",
    "    num_channels=100, \n",
    "    # Training hyper parameter\n",
    "    seed=1337, \n",
    "    learning_rate=0.001, \n",
    "    dropout_p=0.1, \n",
    "    batch_size=128, \n",
    "    num_epochs=20, \n",
    "    early_stopping_criteria=5, \n",
    "    # Runtime option\n",
    "    cuda=True, \n",
    "    catch_keyboard_interrupt=True, \n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    # Dataset params\n",
    "    data_field='job_description', \n",
    "    feature_field='is_fulltime', \n",
    "    is_sequence=True, \n",
    "    sent_embed=True,\n",
    "    vectoriser=OneHotVectoriser,\n",
    "    classifier=CNNEmbedClassifier\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ecc7e66f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-29T14:44:46.304167Z",
     "start_time": "2022-05-29T14:43:23.248340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/document_classification/vectorizer.json\n",
      "\tmodel_storage/document_classification/model.pth\n",
      "Using CUDA: True\n",
      "Not using pre-trained embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013f57a8e9b74d42bf79d849c05112b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7ea1bd98874b3eacfc79bb91800df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd264ade0fd4544aceff3b0950f0a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.390259455029781;\n",
      "Test Accuracy: 92.66826923076924\n"
     ]
    }
   ],
   "source": [
    "do_train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b613aeb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-05-29T14:44:07.520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\t./model_storage/document_classification/vectorizer.json\n",
      "\t./model_storage/document_classification/model.pth\n",
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    name=\"task1\",\n",
    "    news_csv=\"./data/ag_news/news_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"./model_storage/document_classification\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath='/home/dadams/Files/uni/nlp/data/glove.6B/glove.6B.100d.txt', \n",
    "    use_glove=False,\n",
    "    embedding_size=300, \n",
    "    hidden_dim=100, \n",
    "    num_channels=100, \n",
    "    # Training hyper parameter\n",
    "    seed=1337, \n",
    "    learning_rate=0.001, \n",
    "    dropout_p=0.1, \n",
    "    batch_size=100, \n",
    "    num_epochs=5, \n",
    "    early_stopping_criteria=5, \n",
    "    # Runtime option\n",
    "    cuda=True, \n",
    "    catch_keyboard_interrupt=True, \n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    # Dataset params\n",
    "    data_field='job_description', \n",
    "    feature_field='is_fulltime', \n",
    "    is_sequence=True, \n",
    "    sent_embed=False,\n",
    "    vectoriser=PretrainedVectoriser,\n",
    "    classifier=CNNClassifier\n",
    ")\n",
    "do_train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67763d9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Comparison / Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e8591b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "17ee1d9dbca2a2418ac4699e7bcc1e50f21c9d312d9aecaf682b6446d34b368c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
